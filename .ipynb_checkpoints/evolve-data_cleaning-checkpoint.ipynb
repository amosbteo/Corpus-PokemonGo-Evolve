{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# Define the folder path containing the text files\n",
    "folder_path = 'data-raw/'\n",
    "\n",
    "# Instantiate PlaintextCorpusReader with the folder path\n",
    "pokemon_corpus = PlaintextCorpusReader(folder_path, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load text files & Tokenize text into words within sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file IDs (names) in the corpus\n",
    "file_ids = pokemon_corpus.fileids()\n",
    "\n",
    "# Initialize an empty list to store tokenized contents of all files\n",
    "tokenized_corpus = []\n",
    "\n",
    "# Tokenize each file in the corpus\n",
    "for file_id in file_ids:\n",
    "    # Get raw text content of the file\n",
    "    file_content = pokemon_corpus.raw(file_id)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    file_content_lower = file_content.lower()\n",
    "\n",
    "    # Remove symbols using regular expressions\n",
    "    file_content_cleaned = re.sub('(#|\\(|\\)‚Äù)', '', file_content_lower)\n",
    "\n",
    "    # Tokenize the cleaned text content into sentences\n",
    "    sentences = sent_tokenize(file_content_cleaned)\n",
    "    \n",
    "    # Tokenize the text content inside sentences\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Append tokenized content to the tokenized_corpus list\n",
    "    tokenized_corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming tokenized_corpus is your list of lists of lists\n",
    "num_files = len(tokenized_corpus)  # Number of files\n",
    "\n",
    "# Number of rows in each layer (assuming all inner lists have the same length)\n",
    "num_sents = len(tokenized_corpus[0]) if tokenized_corpus else 0\n",
    "\n",
    "print(\"Shape of the tokenized corpus: \", num_files, \"files x\", num_sents, \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subset to only sentences containing the words \"evolve / evolved / evolving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store sentences containing the words \"evolve/evolved/evolving\"\n",
    "tokenized_corpus_evolve = []\n",
    "\n",
    "# Iterate over each document in the tokenized_corpus\n",
    "for file in tokenized_corpus:\n",
    "    # Iterate over each sentence in the document\n",
    "    for sentence_tokens in file:\n",
    "        # Check if any of the keywords are present in the sentence\n",
    "        if any(token in ['evolve', 'evolved', 'evolving'] for token in sentence_tokens):\n",
    "            # Check if \"to\" or \"into\" immediately follow the key tokens\n",
    "            if not any((sentence_tokens[i] in ['from', 'to', 'into', 'him', 'it', '.', '...', ',', ';', ':', '?', '!', '&', '-', '*'] and sentence_tokens[i-1] in ['evolve', 'evolved', 'evolving']) for i in range(1, len(sentence_tokens))):\n",
    "                tokenized_corpus_evolve.append(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of sentences in corpus containing the word \"evolve/evolved/evolving\":', len(tokenized_corpus_evolve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance to get surrounding context to check that \"evolve to\" etc. are not in the data\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "# Convert the list of tokenized sentences into a list of words\n",
    "words = [word for sentence in tokenized_corpus_evolve for word in sentence]\n",
    "\n",
    "# Convert the filtered tokens into an NLTK Text object for contextual analysis\n",
    "evolve_text = nltk.Text(words)\n",
    "\n",
    "# Apply concordance method to the NLTK Text object\n",
    "print(evolve_text.concordance([\"evolve\", \"to\"], width = 100, lines=20))\n",
    "print(evolve_text.concordance([\"evolve\", \"an\"], width = 100, lines=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency analysis\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of sentences\n",
    "flattened_tokens = [token for sublist in tokenized_corpus_evolve for token in sublist]\n",
    "\n",
    "# Remove stopwords + punctuation\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation) + ['evolve', 'evolved', 'evolving'])\n",
    "filtered_tokens = [word for word in flattened_tokens if word not in stop_words]\n",
    "\n",
    "# Perform frequency analysis\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "print(\"Most common words and their frequencies:\")\n",
    "for word, freq in word_freq.most_common(5):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "import csv\n",
    "\n",
    "# Write data to a CSV file\n",
    "with open('data-processed/tokenized_corpus_evolve-checkpoint.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tokenized_corpus_evolve)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Read data from a CSV file\n",
    "with open('data-processed/tokenized_corpus_evolve-checkpoint.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    tokenized_corpus_evolve = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subset corpus to sentences that contain Pokemon name close to \"evolve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a pokemon name list\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "columns_to_read = ['Name', 'Aliases']\n",
    "pokemon_list = pd.read_csv(\"pokemon_list.csv\", usecols = columns_to_read)\n",
    "\n",
    "# Read the columns of interest and flatten the lists into a single list. \n",
    "pokemon_name_singular = pokemon_list['Name'].str.lower().tolist()\n",
    "pokemon_name_singular_alias = pokemon_list['Aliases'].dropna().str.lower().tolist()\n",
    "\n",
    "# Add plural forms to list\n",
    "# Define a list of sibilant endings\n",
    "sibilant_endings = ['s', 'sh', 'ch', 'x', 'z']\n",
    "\n",
    "# Initialize an empty list to store the plural forms\n",
    "pokemon_name_plural = []\n",
    "pokemon_name_plural_alias = []\n",
    "\n",
    "# Loop through each word in the flattened list\n",
    "for word in pokemon_name_singular:\n",
    "    # Check if the word ends with a sibilant\n",
    "    if any(word.endswith(ending) for ending in sibilant_endings):\n",
    "        # Add \"es\" to the word\n",
    "        plural_word = word + 'es'\n",
    "    else:\n",
    "        # Add \"s\" to the word\n",
    "        plural_word = word + 's'   \n",
    "    # Append the plural form to the list\n",
    "    pokemon_name_plural.append(plural_word)\n",
    "\n",
    "for word in pokemon_name_singular_alias:\n",
    "    # Check if the word ends with a sibilant\n",
    "    if any(word.endswith(ending) for ending in sibilant_endings):\n",
    "        # Add \"es\" to the word\n",
    "        plural_word = word + 'es'\n",
    "    else:\n",
    "        # Add \"s\" to the word\n",
    "        plural_word = word + 's'   \n",
    "    # Append the plural form to the list\n",
    "    pokemon_name_plural_alias.append(plural_word)\n",
    "\n",
    "pokemon_name_list = pokemon_name_singular + pokemon_name_plural + pokemon_name_singular_alias + pokemon_name_plural_alias\n",
    "pokemon_name_list[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Initialize an empty list to store words following \"evolve\", \"evolved\", and \"evolving\"\n",
    "words_following_evolve = []\n",
    "\n",
    "# Define the target words\n",
    "target_words = [\"evolve\", \"evolved\", \"evolving\"]\n",
    "\n",
    "# Iterate over the tokenized corpus\n",
    "for i in range(len(tokenized_corpus_evolve) - 1):\n",
    "    for j in range(len(tokenized_corpus_evolve[i])):\n",
    "        # Check if the current token is one of the target words\n",
    "        if tokenized_corpus_evolve[i][j] in target_words and j < len(tokenized_corpus_evolve[i]) - 1:\n",
    "            # Append the word immediately following the target word\n",
    "            words_following_evolve.append(tokenized_corpus_evolve[i][j + 1])\n",
    "\n",
    "# Calculate frequency distribution of words following the target words\n",
    "fdist = FreqDist(words_following_evolve)\n",
    "\n",
    "# Get the most common words\n",
    "most_common_words = fdist.most_common()\n",
    "\n",
    "# Print the most common words vertically\n",
    "for word, frequency in most_common_words:\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get frequency of most common pokemon names that appear immediately after \"evolve/d/ing\"\n",
    "most_common_pokemon = [name for name in most_common_words if name[0] in pokemon_name_list]\n",
    "\n",
    "# Print the most common Pokemon names vertically\n",
    "for pokemon, frequency in most_common_pokemon:\n",
    "    print(f\"{pokemon}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data so that the sentences only contain a Pokemon name within 5 words to the right of \"evolve/d/ing\"\n",
    "# Extract fields for: pokemon name, evolve word form, evolve syntactic frame\n",
    "# Define the target words\n",
    "target_words = [\"evolve\", \"evolved\", \"evolving\"]\n",
    "name_list = pokemon_name_list\n",
    "\n",
    "# Initialize an empty list to store the filtered sentences and names\n",
    "evolve_pokemon_name_in_text = []\n",
    "evolve_word_form = []\n",
    "evolve_frame = []\n",
    "evolve_pokemon_sentences = []\n",
    "\n",
    "# Iterate over each sentence in the tokenized_corpus\n",
    "for sentence in tokenized_corpus_evolve:\n",
    "    # Flag to indicate if the sentence contains any of the target words\n",
    "    contains_target_word = False\n",
    "    \n",
    "    # Iterate over each word index in the sentence\n",
    "    for i, word in enumerate(sentence):\n",
    "        # Check if the word is one of the target words\n",
    "        if word in target_words:\n",
    "            # Check the next five words after the target word, or till the end of the sentence\n",
    "            for j in range(i + 1, min(i + 6, len(sentence)-1)):\n",
    "                # Check if any of the following words are in the Pokemon name list\n",
    "                if sentence[j] in name_list:\n",
    "                    # If any of the following words are in the Pokemon name list, add the pokemon name and the sentence\n",
    "                    evolve_pokemon_name_in_text.append(sentence[j])\n",
    "                    evolve_word_form.append(word)\n",
    "                    evolve_frame.append(sentence[i+1:j])\n",
    "                    evolve_pokemon_sentences.append(sentence)\n",
    "                    contains_target_word = True\n",
    "                    break  # Break out of the inner loop\n",
    "        \n",
    "        if contains_target_word:\n",
    "            break  # Break out of the outer loop if the sentence is included\n",
    "\n",
    "# filtered_sentences now contain sentences that meet the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the list lengths are identical\n",
    "print(len(evolve_pokemon_name_in_text))\n",
    "print(len(evolve_word_form))\n",
    "print(len(evolve_frame))\n",
    "print(len(evolve_pokemon_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance to get surrounding context\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "# Convert the list of tokenized sentences into a list of words\n",
    "# Convert the filtered tokens into an NLTK Text object for contextual analysis\n",
    "# Apply concordance method to the NLTK Text object\n",
    "\n",
    "words = [word for sentence in evolve_pokemon_sentences for word in sentence]\n",
    "evolve_text = nltk.Text(words)\n",
    "evolve_text.concordance([\"evolve\", \"my\", \"first\"], width = 100, lines=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize / Standardize pokemon names\n",
    "\n",
    "# Mapping dictionary for lemmatization\n",
    "plural_mapping = dict(zip(pokemon_name_plural, pokemon_name_singular))\n",
    "additional_mapping = {\n",
    "    \"weeping\": \"weepinbell\",\n",
    "    \"weepingbell\": \"weepinbell\",\n",
    "    \"graveller\": \"graveler\",\n",
    "    \"magicarp\": \"magikarp\",\n",
    "    \"garados\": \"gyarados\",\n",
    "    \"gyrados\": \"gyarados\",\n",
    "    \"ladyba\": \"ledyba\",\n",
    "    \"alteria\": \"altaria\"\n",
    "}\n",
    "plural_mapping.update(additional_mapping)\n",
    "\n",
    "evolve_pokemon_lemma = [plural_mapping.get(name, name) for name in evolve_pokemon_name_in_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_frame = [['EVOLVE'] + sublist + ['POKEMON'] for sublist in evolve_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df = pd.DataFrame(list(zip(evolve_pokemon_lemma, evolve_word_form, evolve_frame, evolve_pokemon_name_in_text, evolve_pokemon_sentences)),\n",
    "               columns =['pokemon_lemma', 'evolve_word_form', 'evolve_frame', 'pokemon_name_in_text', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df.to_csv('data-processed/pokemon_evolve.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
