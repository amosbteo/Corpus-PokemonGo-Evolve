{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "# Define the folder path containing the text files\n",
    "folder_path = 'data-raw/'\n",
    "\n",
    "# Instantiate PlaintextCorpusReader with the folder path\n",
    "pokemon_corpus = PlaintextCorpusReader(folder_path, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load text files & Tokenize text into words within sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file IDs (names) in the corpus\n",
    "file_ids = pokemon_corpus.fileids()\n",
    "\n",
    "# Initialize an empty list to store tokenized contents of all files\n",
    "tokenized_corpus = []\n",
    "\n",
    "# Tokenize each file in the corpus\n",
    "for file_id in file_ids:\n",
    "    # Get raw text content of the file\n",
    "    file_content = pokemon_corpus.raw(file_id)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    file_content_lower = file_content.lower()\n",
    "\n",
    "    # Remove symbols using regular expressions\n",
    "    file_content_cleaned = re.sub('(#|\\(|\\)”)', '', file_content_lower)\n",
    "\n",
    "    # Tokenize the cleaned text content into sentences\n",
    "    sentences = sent_tokenize(file_content_cleaned)\n",
    "    \n",
    "    # Tokenize the text content inside sentences\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Append tokenized content to the tokenized_corpus list\n",
    "    tokenized_corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['post',\n",
       " '1',\n",
       " 'title',\n",
       " ':',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'created',\n",
       " ':',\n",
       " '2018-05-19',\n",
       " '23:50:29',\n",
       " 'id',\n",
       " ':',\n",
       " '8koqq2',\n",
       " 'original',\n",
       " 'post',\n",
       " 'hello',\n",
       " 'everybody',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tokenized corpus:  7 files x 238959 sentences\n"
     ]
    }
   ],
   "source": [
    "# Assuming tokenized_corpus is your list of lists of lists\n",
    "num_files = len(tokenized_corpus)  # Number of files\n",
    "\n",
    "# Number of rows in each layer (assuming all inner lists have the same length)\n",
    "num_sents = len(tokenized_corpus[0]) if tokenized_corpus else 0\n",
    "\n",
    "print(\"Shape of the tokenized corpus: \", num_files, \"files x\", num_sents, \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subset to only sentences containing the words \"evolve / evolved / evolving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store sentences containing the words \"evolve/evolved/evolving\"\n",
    "tokenized_corpus_evolve = []\n",
    "\n",
    "# Iterate over each document in the tokenized_corpus\n",
    "for file in tokenized_corpus:\n",
    "    # Iterate over each sentence in the document\n",
    "    for sentence_tokens in file:\n",
    "        # Check if any of the keywords are present in the sentence\n",
    "        if any(token in ['evolve', 'evolved', 'evolving'] for token in sentence_tokens):\n",
    "            # Check if \"to\" or \"into\" immediately follow the key tokens\n",
    "            if not any((sentence_tokens[i] in ['from', 'to', 'into', 'him', 'it', '.', '...', ',', ';', ':', '?', '!', '&', '-', '*'] and sentence_tokens[i-1] in ['evolve', 'evolved', 'evolving']) for i in range(1, len(sentence_tokens))):\n",
    "                tokenized_corpus_evolve.append(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentences in corpus containing the word \"evolve/evolved/evolving\": 12557\n"
     ]
    }
   ],
   "source": [
    "print('No. of sentences in corpus containing the word \"evolve/evolved/evolving\":', len(tokenized_corpus_evolve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n",
      "None\n",
      "Displaying 20 of 88 matches:\n",
      " be easy might be able to evolve a kingdra . evolve an alteria . etcor that you evolve a seadra int\n",
      "ght away . it 's frustrating enough when you evolve an eevee and do n't get the one you want and th\n",
      " you need the name trick if you 're going to evolve an old eevee . it seems you have to catch new e\n",
      "speon and tamao for umbreonwhenever i try to evolve an evee the game jsut freezes so i have to clos\n",
      "would one want to use 25 jigglypuff candy to evolve an igglybuff to get 1 more jigglypuff ? : d28 s\n",
      " but i 'm going to wait for 20 more candy to evolve an 84 % one . hopefully i 'll walk enough to ev\n",
      " i 'm stoked to use him as my buddy until he evolve an arcanine this list is subject to change , bu\n",
      "n eclipse ? i ’ ll have to set a reminder to evolve an eevee at that exact moment . yes , its my bi\n",
      "rmal wild spawn.the real question is can you evolve an event pikachu into alola raichu and still ke\n",
      "aught is probably omanyte and enough abra to evolve an alakazam.sadly gengar is weak . ok , so it w\n",
      "anging the namedoes it have to be daytime to evolve an espeon with the name trick ? anyone know ? n\n",
      ". in the pokemon games , you are told how to evolve an eevee into umbreon/espeon by npcs . for exam\n",
      "resort , or can i roll the dice and randomly evolve an eevee or do they only give flareon/vaporeon/\n",
      "hanges the outcome . i 'm not certain if you evolve an eevee without lr if the evolution will gain \n",
      "evolve 8 spearow . i know that if you try to evolve an evee you 've walked 10km when your signal is\n",
      "he main issue : it takes 10 km of walking to evolve an eevee . by making it very clear whether it '\n",
      "he main issue : it takes 10 km of walking to evolve an eevee . if you were evolving right before th\n",
      " good way to have more control over when you evolve an eevee ? catch a pokemon regular , lured , in\n",
      "d that i 've been holding onto 400+ candy to evolve an eventual shiny . it would be a long slog if \n",
      " we going to “ get to ” the alolan region to evolve an alolan raichu ? they could also just have th\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Concordance to get surrounding context to check that \"evolve to\" etc. are not in the data\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "# Convert the list of tokenized sentences into a list of words\n",
    "words = [word for sentence in tokenized_corpus_evolve for word in sentence]\n",
    "\n",
    "# Convert the filtered tokens into an NLTK Text object for contextual analysis\n",
    "evolve_text = nltk.Text(words)\n",
    "\n",
    "# Apply concordance method to the NLTK Text object\n",
    "print(evolve_text.concordance([\"evolve\", \"to\"], width = 100, lines=20))\n",
    "print(evolve_text.concordance([\"evolve\", \"an\"], width = 100, lines=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words and their frequencies:\n",
      "pokemon: 3070\n",
      "n't: 2604\n",
      "one: 2377\n",
      "'s: 2177\n",
      "get: 2044\n"
     ]
    }
   ],
   "source": [
    "# Frequency analysis\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of sentences\n",
    "flattened_tokens = [token for sublist in tokenized_corpus_evolve for token in sublist]\n",
    "\n",
    "# Remove stopwords + punctuation\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation) + ['evolve', 'evolved', 'evolving'])\n",
    "filtered_tokens = [word for word in flattened_tokens if word not in stop_words]\n",
    "\n",
    "# Perform frequency analysis\n",
    "word_freq = Counter(filtered_tokens)\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "print(\"Most common words and their frequencies:\")\n",
    "for word, freq in word_freq.most_common(5):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "import csv\n",
    "\n",
    "# Write data to a CSV file\n",
    "with open('data-processed/tokenized_corpus_evolve-checkpoint.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tokenized_corpus_evolve)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Read data from a CSV file\n",
    "with open('data-processed/tokenized_corpus_evolve-checkpoint.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    tokenized_corpus_evolve = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subset corpus to sentences that contain Pokemon name close to \"evolve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bulbasaur', 'ivysaur', 'venusaur', 'mega-venusaur', 'charmander']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a pokemon name list\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "columns_to_read = ['Name', 'Aliases']\n",
    "pokemon_list = pd.read_csv(\"pokemon_list.csv\", usecols = columns_to_read)\n",
    "\n",
    "# Read the columns of interest and flatten the lists into a single list. \n",
    "pokemon_name_singular = pokemon_list['Name'].str.lower().tolist()\n",
    "pokemon_name_singular_alias = pokemon_list['Aliases'].dropna().str.lower().tolist()\n",
    "\n",
    "# Add plural forms to list\n",
    "# Define a list of sibilant endings\n",
    "sibilant_endings = ['s', 'sh', 'ch', 'x', 'z']\n",
    "\n",
    "# Initialize an empty list to store the plural forms\n",
    "pokemon_name_plural = []\n",
    "pokemon_name_plural_alias = []\n",
    "\n",
    "# Loop through each word in the flattened list\n",
    "for word in pokemon_name_singular:\n",
    "    # Check if the word ends with a sibilant\n",
    "    if any(word.endswith(ending) for ending in sibilant_endings):\n",
    "        # Add \"es\" to the word\n",
    "        plural_word = word + 'es'\n",
    "    else:\n",
    "        # Add \"s\" to the word\n",
    "        plural_word = word + 's'   \n",
    "    # Append the plural form to the list\n",
    "    pokemon_name_plural.append(plural_word)\n",
    "\n",
    "for word in pokemon_name_singular_alias:\n",
    "    # Check if the word ends with a sibilant\n",
    "    if any(word.endswith(ending) for ending in sibilant_endings):\n",
    "        # Add \"es\" to the word\n",
    "        plural_word = word + 'es'\n",
    "    else:\n",
    "        # Add \"s\" to the word\n",
    "        plural_word = word + 's'   \n",
    "    # Append the plural form to the list\n",
    "    pokemon_name_plural_alias.append(plural_word)\n",
    "\n",
    "pokemon_name_list = pokemon_name_singular + pokemon_name_plural + pokemon_name_singular_alias + pokemon_name_plural_alias\n",
    "pokemon_name_list[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Initialize an empty list to store words following \"evolve\", \"evolved\", and \"evolving\"\n",
    "words_following_evolve = []\n",
    "\n",
    "# Define the target words\n",
    "target_words = [\"evolve\", \"evolved\", \"evolving\"]\n",
    "\n",
    "# Iterate over the tokenized corpus\n",
    "for i in range(len(tokenized_corpus_evolve) - 1):\n",
    "    for j in range(len(tokenized_corpus_evolve[i])):\n",
    "        # Check if the current token is one of the target words\n",
    "        if tokenized_corpus_evolve[i][j] in target_words and j < len(tokenized_corpus_evolve[i]) - 1:\n",
    "            # Append the word immediately following the target word\n",
    "            words_following_evolve.append(tokenized_corpus_evolve[i][j + 1])\n",
    "\n",
    "# Calculate frequency distribution of words following the target words\n",
    "fdist = FreqDist(words_following_evolve)\n",
    "\n",
    "# Get the most common words\n",
    "most_common_words = fdist.most_common()\n",
    "\n",
    "# Print the most common words vertically\n",
    "for word, frequency in most_common_words:\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get frequency of most common pokemon names that appear immediately after \"evolve/d/ing\"\n",
    "most_common_pokemon = [name for name in most_common_words if name[0] in pokemon_name_list]\n",
    "\n",
    "# Print the most common Pokemon names vertically\n",
    "for pokemon, frequency in most_common_pokemon:\n",
    "    print(f\"{pokemon}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data so that the sentences only contain a Pokemon name within 5 words to the right of \"evolve/d/ing\"\n",
    "# Extract fields for: pokemon name, evolve word form, evolve syntactic frame\n",
    "# Define the target words\n",
    "target_words = [\"evolve\", \"evolved\", \"evolving\"]\n",
    "name_list = pokemon_name_list\n",
    "\n",
    "# Initialize an empty list to store the filtered sentences and names\n",
    "evolve_pokemon_name_in_text = []\n",
    "evolve_word_form = []\n",
    "evolve_frame = []\n",
    "evolve_pokemon_sentences = []\n",
    "\n",
    "# Iterate over each sentence in the tokenized_corpus\n",
    "for sentence in tokenized_corpus_evolve:\n",
    "    # Flag to indicate if the sentence contains any of the target words\n",
    "    contains_target_word = False\n",
    "    \n",
    "    # Iterate over each word index in the sentence\n",
    "    for i, word in enumerate(sentence):\n",
    "        # Check if the word is one of the target words\n",
    "        if word in target_words:\n",
    "            # Check the next five words after the target word, or till the end of the sentence\n",
    "            for j in range(i + 1, min(i + 6, len(sentence)-1)):\n",
    "                # Check if any of the following words are in the Pokemon name list\n",
    "                if sentence[j] in name_list:\n",
    "                    # If any of the following words are in the Pokemon name list, add the pokemon name and the sentence\n",
    "                    evolve_pokemon_name_in_text.append(sentence[j])\n",
    "                    evolve_word_form.append(word)\n",
    "                    evolve_frame.append(sentence[i+1:j])\n",
    "                    evolve_pokemon_sentences.append(sentence)\n",
    "                    contains_target_word = True\n",
    "                    break  # Break out of the inner loop\n",
    "        \n",
    "        if contains_target_word:\n",
    "            break  # Break out of the outer loop if the sentence is included\n",
    "\n",
    "# filtered_sentences now contain sentences that meet the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3686\n",
      "3686\n",
      "3686\n",
      "3686\n"
     ]
    }
   ],
   "source": [
    "# Check the list lengths are identical\n",
    "print(len(evolve_pokemon_name_in_text))\n",
    "print(len(evolve_word_form))\n",
    "print(len(evolve_frame))\n",
    "print(len(evolve_pokemon_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 20 of 23 matches:\n",
      "into vaporeon . finally scraped enough to evolve my first eevee , got a 700cp jolteon . i 've only \n",
      "efore tonight it took me since release to evolve my first gengar 2 days ago . edit2 : as someone po\n",
      "a lot of walking to get enough candies to evolve my first larvitar , only for my tyranitar to know \n",
      ". i 've only just scraped enough candy to evolve my first piloswine : p i play a lot ) not anymore \n",
      " 'm unhappy that i 'll finally be able to evolve my first dragonite a little sooner . good to know \n",
      "felt when i finally had enough candies to evolve my first charizard . alolan raichu comes from evol\n",
      " the tweet . i just got enough candies to evolve my first dragonite less than a week ago . i 'm gon\n",
      " evolve 2 omastars . enough karp candy to evolve my first gyrados , ive got a 93 % er thats going t\n",
      "quest . finally was able to get enough to evolve my first magikarp 3 months ago . i ’ m frustrated \n",
      "p ! i needed to catch one more dratini to evolve my first dragonite , did n't see any for weeks , a\n",
      " walking my machoke for ~100km to finally evolve my first machamp was not even satisfying when i di\n",
      "age 5.3x the candy . one day before i did evolve my first magikarp ... but now i am really mad beca\n",
      "ved my first espeon with the name , can i evolve my first umbreon with the name , or do all of them\n",
      "is already goneoh , oops . ca n't wait to evolve my first kingdra i 've got so many seadra all line\n",
      "arp ... so many ... i was finally able to evolve my first gyarados last night . said his goal was t\n",
      "till working on getting enough candies to evolve my first butterfree ... im getting 10 weedles for \n",
      "barely saw pidgey and i was so excited to evolve my first pidgeot . imagine evolving a 500cp magica\n",
      "ght me . i am almost at enough candies to evolve my first eevee and have been wondering which one t\n",
      " hatched a karp and had enough candies to evolve my first gyarados . i wont even evolve a growlithe\n",
      " 5 months i finally got enough candies to evolve my first gyrados . i have evolved three gyarados .\n"
     ]
    }
   ],
   "source": [
    "# Concordance to get surrounding context\n",
    "from nltk.text import ConcordanceIndex\n",
    "\n",
    "# Convert the list of tokenized sentences into a list of words\n",
    "# Convert the filtered tokens into an NLTK Text object for contextual analysis\n",
    "# Apply concordance method to the NLTK Text object\n",
    "\n",
    "words = [word for sentence in evolve_pokemon_sentences for word in sentence]\n",
    "evolve_text = nltk.Text(words)\n",
    "evolve_text.concordance([\"evolve\", \"my\", \"first\"], width = 100, lines=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize / Standardize pokemon names\n",
    "\n",
    "# Mapping dictionary for lemmatization\n",
    "plural_mapping = dict(zip(pokemon_name_plural, pokemon_name_singular))\n",
    "additional_mapping = {\n",
    "    \"weeping\": \"weepinbell\",\n",
    "    \"weepingbell\": \"weepinbell\",\n",
    "    \"graveller\": \"graveler\",\n",
    "    \"magicarp\": \"magikarp\",\n",
    "    \"garados\": \"gyarados\",\n",
    "    \"gyrados\": \"gyarados\",\n",
    "    \"ladyba\": \"ledyba\",\n",
    "    \"alteria\": \"altaria\"\n",
    "}\n",
    "plural_mapping.update(additional_mapping)\n",
    "\n",
    "evolve_pokemon_lemma = [plural_mapping.get(name, name) for name in evolve_pokemon_name_in_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (611969434.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    evolve_frame = [[\"EVOLVE'] + sublist + ['POKEMON'] for sublist in evolve_frame]\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "evolve_frame = [['EVOLVE'] + sublist + ['POKEMON'] for sublist in evolve_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df = pd.DataFrame(list(zip(evolve_pokemon_lemma, evolve_word_form, evolve_frame, evolve_pokemon_name_in_text, evolve_pokemon_sentences)),\n",
    "               columns =['pokemon_lemma', 'evolve_word_form', 'evolve_frame', 'pokemon_name_in_text', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolve_df.to_csv('data-processed/pokemon_evolve.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
